# RAG-ассистент Сбербанка — отчёт

## Описание проекта
Телеграм-бот, который с помощью LangChain и Retrieval-Augmented Generation отвечает на вопросы клиентов Сбербанка про карты, кредиты и вклады. Источники знаний: PDF-документы из `data/` и JSON-FAQ, ответы формируются с учётом истории диалога и трансформации запросов.

## Вариант задания
Расширенный вариант: помимо базового RAG добавлены query transformation, JSON-слой с быстрыми ответами, fallback по FAQ, поддержка двух провайдеров моделей (OpenRouter и Fireworks), отдельный CLI для регрессионных тестов и подробное логирование.

## Реализованные возможности
- [x] Telegram-бот на aiogram 3 с командами `/start`, `/help`, `/index`, `/index_status`
- [x] Автоиндексация PDF при старте, переиндексация по команде и отслеживание статуса
- [x] Поддержка JSON-FAQ (`data/sberbank_help_documents.json`) с прямыми ответами через `faq_lookup`
- [x] Query Transformation для уточняющих вопросов и хранение истории в формате LangChain messages
- [x] Асинхронная обработка, логирование в `logs/bot.log`, контроль rate limit
- [x] Локальный сценарий `src/local_test.py` для проверки индекса/LLM без Telegram
- [x] Конфигурация через `.env` с возможностью переключения провайдеров/моделей

## Технологический стек
Python 3.11, uv, aiogram 3, LangChain + langchain-openai, OpenAI-compatible API (OpenRouter/Fireworks), PyPDF, RecursiveCharacterTextSplitter, InMemoryVectorStore, python-dotenv, httpx, logging.

## Используемые модели
| Роль | Модель | Провайдер / комментарий |
| --- | --- | --- |
| Основная LLM | `openai/gpt-oss-20b:free` по умолчанию, опция `accounts/fireworks/models/gpt-oss-120b` | Задаётся переменной `MODEL`, работает через ChatOpenAI-интерфейс |
| Query transform | `gpt-4o` (OpenAI-сумместимая) | Отдельная модель `MODEL_QUERY_TRANSFORM` для стабильных уточнений |
| Эмбеддинги (OpenRouter) | `text-embedding-3-large` | 3072-мерные вектора, максимальная точность, но самая высокая задержка |
| Эмбеддинги (Fireworks) | `accounts/fireworks/models/qwen3-embedding-8b` | Быстрее и лучше понимает русские формулировки, использовалась во втором прогоне |

## Эксперименты с индексацией
Методика: PDF из `data/` (29 страниц) разбиваются на чанки, затем прогоняются контрольные вопросы из `src/local_test.py`. Для каждой конфигурации фиксировали количество чанков, среднюю длину и качество поиска (ручная проверка top-k=3).

| Конфигурация | chunk_size / overlap | Кол-во чанков | Средняя длина, симв | Наблюдения |
| --- | --- | --- | --- | --- |
| Базовое разделение (`src/indexer.py`) | 800 / 100 | 246 | 723 | Мало чанков → retriever часто подтягивает смешанные ответы (карты + вклады) и теряет короткие FAQ. 2 из 7 вопросов вернули нерелевантный первый чанк. |
| Конфигурация из README | 500 / 50 | 377 | 437 | Добавилось 53% чанков, выросла полнота, но списки и таблицы всё равно рвутся посередине, поэтому ответы про документы на карту иногда теряют начало. |
| Текущая прод-конфигурация (`src/indexer_with_json.py`) | 450 / 120 + кастомные разделители (`\n\n`, маркеры списков, `.`) | 519 | 387 | Больше перекрытия и семантические разделители → стабильный top-k, 7/7 релевантных ответов. Цена — рост памяти, но InMemoryVectorStore справляется. |

**Вывод:** для банковских документов лучше стратегия с меньшими чанками (≈450 символов), большим overlap (120) и явными разделителями для списков. Она удерживает маркеры «1)», «•» и не допускает разрыва юридических формулировок.

## Работа с JSON датасетом
### Загрузка JSON (JSONLoader)
В `src/indexer_with_json.py` реализована функция `load_json_documents()`:
- валидирует `data/sberbank_help_documents.json`, пропускает битые записи и дубликаты по паре «вопрос-ответ»;
- добавляет метаданные (`category`, `url`, `type`, нормализованный вопрос) и формирует чанк вида `Вопрос/Ответ`;
- результаты кешируются в `faq_lookup` для быстрого поиска по похожести (`SequenceMatcher` с порогом 0.82).

### Интеграция с RAG
При индексации PDF-чанки объединяются с JSON-чанками, а `faq_lookup` отдаёт мгновенный ответ, если текущий вопрос совпадает с FAQ. Это видно в `rag.py`: перед запуском retriever проверяется FAQ-мэтч, после чего LLM получает уже готовый контекст.

### Скриншот с вопросами про карты
[Диалог «Как заказать карту?»](screenshots/cards-qa.png) — демонстрация: после команды `/index` бот отвечает на вопросы про выпуск и документы, используя JSON-данные и свежий индекс.

## Сравнение моделей эмбеддингов
Контрольный набор — 7 вопросов из `src/local_test.py` (карты, кредиты, вклады). Оценка: вручную проверяли, попал ли релевантный чанк в top-1 и насколько естественен ответ LLM.

| Модель эмбеддингов | Провайдер | Точность @1 (7 вопросов) | Замечания |
| --- | --- | --- | --- |
| `text-embedding-3-large` | OpenRouter/OpenAI | 6 / 7 | Богатые вектора, но иногда хуже схлопывают синонимы («какие документы» vs «что нужно, чтобы получить карту»). Ответы формальные, требуется больше подсказок для FAQ. |
| `qwen3-embedding-8b` | Fireworks | 7 / 7 | Лучше «понимает» русские вариации, особенно в блоке карт. Снижает потребность в FAQ fallback и ускоряет индексацию (~15% меньше времени из-за 2048-мерных векторов). |

**Вывод:** для русскоязычных банковских документов предпочтительнее Fireworks `qwen3-embedding-8b` — он точнее на пользовательских формулировках и даёт более чистый контекст для LLM.

